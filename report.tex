\documentclass[10pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{cite}
\usepackage[margin=1in]{geometry}
\usepackage{float}
\usepackage{booktabs}
\usepackage{tcolorbox}
\tcbuselibrary{minted}

\title{Video Summarization with NLP and Vision-Language Models}
\author{
Bhargava Siva Naga Sai Potluri(bxp230045), \\
Kavimayil Periyakoravampalayam Komarasamy (kxp230053),\\
Nikhil Sesha Sai Kondapalli(nxk240025), 
Sakshi Tokekar(sxt230143)\\
\text{Group 16}\\
\texttt{https://github.com/nikhilkondapalli5/LLMVS-Implementation}
}
\date{}

\begin{document}

\maketitle

\begin{abstract}
We present an implementation of the LLMVS (LLM-based Video Summarization) framework that leverages Large Language Models for video summarization. Our approach uses a Multi-modal LLM to generate textual descriptions of video frames, followed by an LLM to evaluate frame importance within local contexts. A self-attention mechanism aggregates global context for final predictions. Experiments on the TVSum dataset demonstrate superior performance (Kendall's $\tau$=0.177, Spearman's $\rho$=0.229) compared to baseline methods, validating the effectiveness of combining semantic understanding with LLM reasoning.
\end{abstract}

\section{Introduction}

The exponential growth of video content across digital platforms has created significant challenges in efficient content navigation, search, and retrieval. Millions of videos are uploaded daily, far exceeding human capacity for consumption and manual processing. This information overload necessitates advanced video summarization techniques that can automatically condense lengthy videos into concise summaries while preserving essential content and narrative structure.

Traditional video summarization methods rely heavily on visual features and temporal dynamics, often failing to capture the semantic content and narrative structure of videos. Recent multi-modal approaches integrate both visual and language modalities but still prioritize visual features, with textual data serving primarily as auxiliary information to enhance visual representations.

Inspired by the work of Lee et al.~\cite{lee2025video} on "Video Summarization with Large Language Models," our project introduces the LLMVS framework. Unlike traditional methods, our approach leverages the semantic understanding and contextual reasoning capabilities of Large Language Models. We employ a Multi-modal Large Language Model (LLaVA-1.5-7B)  to generate textual descriptions of video frames, followed by an LLM (Llama-2-13B)~\cite{touvron2023llama} to evaluate frame importance within local temporal contexts.

A key innovation in our methodology is the use of LLM output embeddings rather than direct text answers, which allows us to extract richer semantic information. These local importance scores are then refined through a self-attention mechanism that captures global context across the entire video sequence, ensuring summaries effectively reflect both details and the overarching narrative.

We conducted experiments on the TVSum dataset, which contains 50 videos spanning diverse genres including how-to videos, documentaries, and vlogs. Our implementation utilized a train-test split of 40 training videos and 10 test videos. The evaluation employed ranking metrics (Kendall's $\tau$ and Spearman's $\rho$) to measure the correlation between predicted and ground truth importance scores. Our results demonstrate that the LLMVS framework achieves superior performance, outperforming both the A2Summ method and the zero-shot LLM baseline, validating the effectiveness of combining textual understanding with LLM reasoning for video summarization tasks.

\section{Related Work}

Video summarization has evolved from early methods relying on low-level visual features (e.g., color histograms, motion) to deep learning approaches utilizing RNNs and Transformers to model temporal dependencies. While effective, these methods often struggle to capture high-level semantic concepts.

Recent advancements have shifted towards multi-modal approaches. The integration of vision-language models has enabled better semantic alignment between visual content and textual summaries. For instance, Argaw et al.~\cite{argaw2024scaling} demonstrated the potential of scaling up video summarization datasets using LLMs as "oracle" annotators, significantly improving model generalization. Similarly, Mu et al.~\cite{mu2024moe} proposed a Mixture of Experts framework to leverage multiple VideoLLMs for comprehensive summary generation.

Our work builds upon the LLMVS framework~\cite{lee2025video}, which uniquely employs LLM output embeddings for importance scoring. Unlike real-time approaches such as Aha~\cite{chang2025aha} that focus on streaming data, LLMVS and our implementation prioritize global context understanding through self-attention, ensuring the final summary respects the entire video's narrative structure.

\section{Data}

\subsection{Dataset Overview}

For our implementation, we utilized the TVSum (TV Summary) dataset~\cite{song2015tvsum}, a well-established benchmark for video summarization research. The TVSum dataset comprises 50 videos with varying durations and content types, making it ideal for evaluating the generalization capabilities of video summarization models.

\subsection{Dataset Characteristics}

\textbf{Size and Duration:} The dataset contains 50 videos with durations ranging from 2 to 10 minutes, averaging 4 minutes and 11 seconds per video. The content spans diverse genres including how-to videos, documentaries, vlogs, news clips, and various other categories.

\textbf{Annotations:} Each video is annotated by 20 independent raters who assign importance scores on a continuous scale from 0 to 1. Ground truth scores are provided at the segment level by averaging these user annotations. This multi-rater scheme captures the inherent subjectivity in defining keyframes while providing robust ground truth.

\subsection{Data Preprocessing}

Prior to feature extraction, the videos were segmented into semantically consistent shots using Kernel Temporal Segmentation (KTS). These pre-computed change points serve as the boundaries for our segment-level importance scoring and ensure that the generated summaries respect natural scene transitions rather than arbitrary fixed-time windows.

\subsection{Data Structure}

Each video in the dataset is accompanied by metadata that includes:

\begin{itemize}
    \item \textbf{picks}: Sampled frame indices used for feature extraction
    \item \textbf{features}: Pre-extracted visual features using GoogLeNet pool5 layer (dimension: 1024)
    \item \textbf{gtscore}: Ground truth importance scores averaged across user annotations
    \item \textbf{n\_frame\_per\_seg}: Number of frames in each video segment
    \item \textbf{change\_points}: Start and end frame indices for each segment (derived via KTS)
\end{itemize}

As an example, Video ID 38 has a length of 94 seconds, contains 2,941 total frames at 30 fps, with 197 sampled feature steps and 20 segments. The dataset's diversity in content types and the availability of multiple human annotations make it particularly suitable for evaluating the semantic understanding capabilities of our LLM-based approach, as the ground truth reflects human judgment about frame importance rather than purely visual saliency.
\section{Methodology}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{arch.png}
  \caption{Overall architecture: Input frames are captioned by an M-LLM and passed to an LLM, which estimates frame importance using neighboring context. The resulting embeddings are reduced via an MLP, refined with self-attention to capture global context, and used to produce final importance scores for each frame.}
  \label{fig:arch}
\end{figure*}

Our methodology is inspired by recent LLM-based video summarization frameworks and consists of three main stages (Figure~\ref{fig:arch}): frame caption generation, local importance scoring, and global context aggregation.

\subsection{Frame Caption Generation}

First, each video is decomposed into a sequence of frames. A pre-trained multimodal model(LLaVA-1.5-7B) is used to generate a short natural language description for each frame. These captions aim to capture the main action, objects, and scene context present in the frame, effectively translating visual information into the language domain. This transformation enables us to leverage the semantic understanding capabilities of language models for subsequent video analysis.

\subsection{Local Importance Scoring}

Next, we perform local importance scoring by evaluating each frame within a temporal window of neighboring frames. For a given frame, captions from a fixed-size sliding window are grouped together and provided as input to a language model(Llama-2-13B). The model is prompted to assess the importance of the central frame based on criteria such as narrative relevance, uniqueness, and action intensity. 

Instead of relying only on the final numerical output, we extract intermediate language model embeddings that preserve richer semantic information. These embeddings capture the contextual understanding developed by the language model during its evaluation process, encoding both the query context and the model's assessment in a high-dimensional representation.

\subsection{Global Context Aggregation}

Finally, to ensure global coherence across the entire video, we aggregate the locally computed representations using a self-attention-based module. This global aggregation step allows the model to capture long-range dependencies and avoid redundant frame selection. The self-attention mechanism enables the model to weigh the importance of each frame relative to all other frames in the video, ensuring that the final summary reflects the overall narrative structure.

The output is a sequence of importance scores, one per frame, which can be used to generate a final video summary by selecting the most informative segments under a length constraint. During training, only the global aggregation module is updated, while the pre-trained multimodal and language models remain frozen to preserve their general knowledge.

\subsection{Training objective}

The proposed method is trained using the Mean Squared Error (MSE) loss to optimize frame importance predictions. The loss L between the ground truth score vector $\hat{s}$ and the predicted score s and is defined as:

\[
\mathcal{L} = \frac{1}{T} \sum_{t=1}^{T} \left( s_t - \hat{s}_t \right)^2
\]

\subsection{Summary Generation}

To generate the final video summary, we formulate the selection problem as a 0/1 Knapsack problem. We aim to maximize the total importance score of the selected segments such that their combined duration does not exceed a pre-defined limit (typically 15\% of the original video length).

Let $v_i$ be the importance score of segment $i$, $w_i$ be its duration, and $C$ be the maximum allowed duration. We solve for binary variables $x_i \in \{0, 1\}$ to:

\begin{equation}
    \text{maximize} \sum_{i=1}^{N} v_i x_i \quad \text{subject to} \quad \sum_{i=1}^{N} w_i x_i \leq C
\end{equation}

This ensures that the generated summary contains the most semantically significant moments while strictly adhering to the length constraint.

\section{Implementation}
\subsection{Training Infrastructure}
Training was conducted on a single NVIDIA A100 GPU using Google Colab, with a training time of 3.5 hours per dataset split. With multiple GPUs, k-fold cross-validation could be implemented using distributed training across all splits simultaneously.


\subsection{Architecture Implementation}
Our trainable architecture consists of three main components that process the LLM embeddings to produce final importance scores:

\textbf{Dimension Reduction Module:} The 5120-D embeddings from Llama-2-13B are reduced to 2048-D using adaptive max pooling along the sequence dimension, followed by a linear projection and layer normalization for training stability.

\textbf{Global Context Aggregator:} A 3-layer transformer encoder (d\_model=2048, nhead=2) processes the reduced embeddings. Each layer contains multi-head self-attention and feed-forward networks with residual connections and layer normalization. This enables each frame to attend to all other frames, capturing long-range dependencies and global narrative structure.

\textbf{Score Prediction Head:} A 5-layer MLP progressively reduces dimensionality from 2048 to 1, with layer normalization applied at intermediate stages (after //2 and //8 reductions) for gradient stability. The final sigmoid activation produces importance scores in the (0,1) range.

The complete PyTorch implementation is shown below:

\begin{tcblisting}{
    listing only,
    colback=white,
    colframe=black,
    boxrule=0.5pt,
    arc=0pt,
    left=15pt,
    minted language=python,
    minted options={
        fontsize=\scriptsize,
        linenos,
        breaklines,
        numbersep=5pt
    }
}
# Dimension Reduction Module
self.d_max_pooling = nn.AdaptiveMaxPool1d(self.config.reduced_dim)
self.d_linear1 = nn.Linear(5120, self.config.reduced_dim)
self.d_linear1_norm = nn.LayerNorm(self.config.reduced_dim)
self.c_max_pooling = nn.AdaptiveMaxPool1d(1)

# Global Context Aggregator (Self-Attention Transformer)
encoder_layer_agg = nn.TransformerEncoderLayer(
    d_model=self.config.reduced_dim, 
    nhead=self.config.num_heads, 
    batch_first=True
)
self.transformer_encoder_agg = nn.TransformerEncoder(
    encoder_layer_agg, 
    num_layers=self.config.num_layers
)

# Score Prediction Head (Final MLP)
self.mlp_head = torch.nn.Sequential(
    nn.Linear(self.config.reduced_dim, self.config.reduced_dim//2),
    nn.LayerNorm(self.config.reduced_dim // 2), 
    nn.ReLU(),
    nn.Linear(self.config.reduced_dim//2, self.config.reduced_dim//4),
    nn.ReLU(),
    nn.Linear(self.config.reduced_dim//4, self.config.reduced_dim//8),
    nn.LayerNorm(self.config.reduced_dim // 8),
    nn.ReLU(),
    nn.Linear(self.config.reduced_dim//8, self.config.reduced_dim//16),
    nn.ReLU(),
    nn.Linear(self.config.reduced_dim//16, 1),
    nn.Sigmoid()
)
\end{tcblisting}

\subsection{Optimization Strategy}
The model is trained end-to-end using MSE loss between predicted and ground truth importance scores. We employ the AdamW optimizer with adaptive per-parameter learning rates and decoupled weight decay for improved generalization.

\begin{tcblisting}{
    listing only,
    colback=white,
    colframe=black,
    boxrule=0.5pt,
    arc=0pt,
    left=15pt,
    minted language=python,
    minted options={
        fontsize=\scriptsize,
        linenos,
        breaklines,
        numbersep=5pt
    }
}
def configure_optimizers(self):
    optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.lr)
    lr_scheduler = {
        'scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=100, eta_min=1e-6
        ),
        'interval': 'epoch',
        'frequency': 1,
    }
    return [optimizer], [lr_scheduler]
\end{tcblisting}

\subsection{Hyperparameters}
\begin{itemize}
    \item Reduced dimension: 2048
    \item Attention heads: 2
    \item Transformer layers: 3
    \item Training epochs: 200
    \item Learning rate: 7e-5
    \item Optimizer: AdamW
    \item Batch size: 1
\end{itemize}

Note that only the trainable components (dimension reduction, transformer encoder, and MLP head) are updated during training. The pre-trained LLaVA-1.5-7B and Llama-2-13B models remain frozen to preserve their learned semantic understanding.

\section{Experiments and Results}

\subsection{Data Split}

For our experiments, we adopted the following data partitioning strategy: 40 videos (80\%) for training and 10 videos (20\%) for testing. This split allows adequate training data while maintaining sufficient test samples for a reliable performance evaluation.

\subsection{Importance Score Evaluation}
We evaluated the quality of predicted importance scores by measuring agreement between the induced ranking of segments and the reference ranking using Kendall's $\tau$ and Spearman's $\rho$ (higher is better). Kendall's $\tau$ reflects pairwise ordering consistency, while Spearman's $\rho$ measures global monotonic agreement between ranked lists. Baselines marked with * follow the methods reported in the reference work~\cite{lee2025video}.

\begin{table}[H]
\centering
\caption{Comparison of importance scores}
\label{tab:importance_scores}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Kendall's $\tau$} & \textbf{Spearman's $\rho$} \\
\midrule
A2Summ*       & 0.137 & 0.165 \\
LLM*          & 0.051 & 0.056 \\
Our Approach  & 0.177 & 0.229 \\
\bottomrule
\end{tabular}\\[0.5em]
\footnotesize
*Methods are based on the reference paper~\cite{lee2025video}
\end{table}

\paragraph{Results.}
As shown in Table~\ref{tab:importance_scores}, our approach achieves the strongest alignment with the reference ranking ($\tau=0.177$, $\rho=0.229$), outperforming A2Summ* ($\tau=0.137$, $\rho=0.165$) and LLM* ($\tau=0.051$, $\rho=0.056$).

\paragraph{Analysis.}
Our method improves over A2Summ* by $+0.040$ in Kendall's $\tau$ and $+0.064$ in Spearman's $\rho$, indicating fewer pairwise inversions and better overall rank consistency. The gains over LLM* are larger ($+0.126$ in $\tau$ and $+0.173$ in $\rho$), suggesting that our design produces more reliable and globally consistent importance rankings than a direct LLM-based baseline from~\cite{lee2025video}. While absolute correlations remain moderate, the consistent improvement across both rank metrics demonstrates that our approach better captures the relative importance structure needed for summarization.

\subsection{Response-level Video Summary Evaluation}
In addition to importance-rank agreement, we compare the generated summaries directly by measuring similarity between the LLM-generated response (baseline) and our approach's response on a per-video basis. We reported lexical overlap metrics (ROUGE-1/2/L F1~\cite{lin2004rouge}) and semantic similarity metrics (BERTScore F1~\cite{zhang2019bertscore} and embedding cosine similarity). ROUGE reflects n-gram overlap, whereas BERTScore and cosine similarity better capture paraphrasing and semantic equivalence.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\linewidth]{average_metrics.png} 
  \caption{Average (across videos) comparison of generated summaries using ROUGE-1/2/L F1, BERTScore F1, and cosine similarity.}
  \label{fig:avg_summary_eval}
\end{figure}

\paragraph{Results and trends.}
Figure~\ref{fig:avg_summary_eval} summarizes the \emph{average} performance across all videos. This aggregate view provides the main conclusion of our response-level evaluation: our approach achieves consistently strong semantic agreement while maintaining competitive lexical overlap. Figure~\ref{fig:video_summary_eval} shows per-video performance across all metrics. Overall, semantic metrics (BERTScore and cosine similarity) remain consistently high across videos, indicating that our summaries preserve the core meaning even when wording differs. In contrast, ROUGE scores exhibit larger variance across video IDs, which is expected because small surface-form differences can significantly affect n-gram overlap.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{per_video_metrics.png}
  \caption{Per-video comparison of test-set video summaries using ROUGE-1/2/L F1, BERTScore F1, and cosine similarity.}
  \label{fig:video_summary_eval}
\end{figure}

\paragraph{Analysis.}
The gap between strong semantic similarity and more variable ROUGE suggests that our approach often produces semantically aligned but lexically different responses compared to the baseline. This is desirable in summarization settings where multiple valid phrasings exist. We also observe that ROUGE-2 is the most sensitive metric (largest fluctuations), reflecting its reliance on exact bigram matches; ROUGE-1 and ROUGE-L are comparatively more stable due to unigram and sequence-level matching. Finally, per-video variation highlights that summary difficulty is content-dependent, motivating the use of both lexical and semantic metrics for a balanced evaluation. 



\subsection{Error Analysis: Low ROUGE but Correct Semantics}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Error_Analysis.png}
    \caption{Example where ROUGE underestimates summary quality: the model paraphrases the reference and omits minor details, but preserves the core semantics and key takeaway.}
    \label{fig:error_analysis_rouge}
\end{figure*}
Figure~\ref{fig:error_analysis_rouge} illustrates a representative failure case where the ROUGE score is relatively low even though the generated summary preserves the main meaning of the original video. This happens because ROUGE is primarily an n-gram overlap metric, so paraphrasing and lexical variation (e.g., using different words for the same idea) can reduce the score despite semantic equivalence. In this example, the model output correctly captures the key semantics: the spring maintenance context, the focus on paper wasp nests commonly found under eaves, and the recommendation to remove or knock down nests early before colonies become established. However, the model abstracts and omits secondary details (e.g., that paper wasps can be beneficial insects, and specific guidance about chemical control and timing), which further decreases lexical overlap and therefore ROUGE. This observation highlights the limitation of overlap-based metrics alone and motivates complementing ROUGE with semantic similarity measures (e.g., BERTScore/cosine similarity) and qualitative or human evaluation.

\section{Conclusion}

In this project, we successfully implemented the LLMVS framework for video summarization, demonstrating the effectiveness of leveraging Large Language Models for understanding and summarizing video content. Our approach achieves superior performance on the TVSum dataset compared to baseline methods, with Kendall's $\tau$ of 0.177 and Spearman's $\rho$ of 0.229, substantially outperforming the A2Summ benchmark ($\tau$=0.137, $\rho$=0.165) and the zero-shot LLM baseline ($\tau$=0.051, $\rho$=0.056).

\subsection{Key Findings}

\textbf{Effectiveness of LLM Embeddings:} Our results validate that extracting and utilizing output embeddings from LLMs provides richer semantic information compared to using direct textual answers. This approach enables better capture of contextual relationships and narrative structure within videos.

\textbf{Importance of Global Context:} The significant performance improvement over the zero-shot LLM baseline highlights the critical role of the global context aggregator. The self-attention mechanism successfully integrates information across the entire video sequence, enabling coherent and contextually aware summarization decisions.

\textbf{Semantic Understanding:} By centering the summarization process around textual descriptions and LLM reasoning rather than purely visual features, our framework demonstrates strong semantic understanding capabilities. The model effectively identifies action-oriented content and key narrative moments that align with human judgment.

\textbf{Qualitative Analysis:} Our generated summaries show good alignment with ground truth annotations, particularly in identifying important action sequences and minimizing redundant content. The model successfully captures high-importance segments such as demonstrations in how-to videos and dynamic actions in event footage.

\subsection{Future Directions}

While our results are promising, several areas warrant future exploration. First, extending evaluation to diverse datasets beyond TVSum (such as SumMe) would better establish generalization capabilities. Second, incorporating audio modality during summarization could enrich contextual understanding, particularly for videos with significant spoken content or music. Third, experimenting with different M-LLMs and LLMs could potentially improve both efficiency and effectiveness. Finally, investigating more sophisticated attention mechanisms or exploring larger window sizes might further enhance performance.

In conclusion, this project demonstrates that Large Language Models, when properly integrated into video processing pipelines, can significantly advance video summarization capabilities by bringing semantic understanding and contextual reasoning to complement traditional visual analysis. The LLMVS framework represents a promising direction for future multimedia content analysis research, bridging the gap between visual data and language-based reasoning.

    \bibliographystyle{plain} % or 'ieeetr' for IEEE style
    \bibliography{references} % Points to references.bib

\end{document}
